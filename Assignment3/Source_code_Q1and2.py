# -*- coding: utf-8 -*-
"""FML_Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bbh1QnD8E85D_NoWPJZKFdYtnH6ap86s
"""

# Libraries
import pandas as pd
from PIL import Image
import io
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings('ignore')

"""###### Download the MNIST dataset from https://huggingface.co/datasets/mnist. Use a random set of 1000 images (100 from each class 0-9) as your dataset."""

np.random.seed(5400)
splits = {'train': 'mnist/train-00000-of-00001.parquet', 'test': 'mnist/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/ylecun/mnist/" + splits["train"])
data = df.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 100)))
print(data['label'].value_counts())
data

# Visualizing a single image
image = data.iloc[0]["image"]['bytes']
image = Image.open(io.BytesIO(image))
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()

"""###### Write a piece of code to run the PCA algorithm on this data-set. Visualize the images of the principal components that you obtain. How much of the variance in the data-set is explained by each of the principal components?"""

def image_array(image_bits):
    img = Image.open(io.BytesIO(image_bits))
    image_1D = np.array(img)
    return image_1D.flatten()

data['image_numerical'] = data['image'].apply(lambda x: image_array(x['bytes']))

# Centering
X = np.stack(data['image_numerical'].values)
mean_X = np.mean(X, axis=0)
X_centered = X - mean_X

# Covariance Matrix
covariance_matrix = (X_centered.T @ X_centered) / (X_centered.shape[0] - 1)
covariance_matrix = (covariance_matrix + covariance_matrix.T) / 2
e_values, e_vectors = np.linalg.eig(covariance_matrix)
e_values = np.real(e_values)

# Top eignvalues
idx = np.argsort(e_values)[::-1]
sort_e_vectors = e_vectors[:, idx]
sort_e_values = e_values[idx]
total_variance = np.sum(sort_e_values)
variance_explained = sort_e_values / total_variance * 100
cum_variance = np.cumsum(variance_explained)
idx95 = np.argmax(cum_variance >= 95) + 1
print(f'The number of PCs required for 95 % explained variance = {idx95}')

var = variance_explained.tolist()
cumvar = cum_variance.tolist()

# Plotting
plt.figure(figsize=(14, 6))
plt.bar(range(1, idx95 + 1), variance_explained[:idx95], alpha=0.7, label='Variance Explained', color='blue')

plt.xlabel('Principal Component')
plt.ylabel('Percentage of Variance Explained')
plt.title('Variance Explained by Principal Components')
plt.legend(loc='best')
plt.grid(True)
plt.show()

pca_df = pd.DataFrame({
    'Principal Component': range(1, len(variance_explained) + 1),
    'Variance Explained (%)': variance_explained,
    'Cumulative Variance Explained (%)': cumvar })
pca_df.head(25)

k = 25
topkeigenvectors = sort_e_vectors[:, :k]
topkeigenvectors = np.real(topkeigenvectors)

plt.figure(figsize=(15, 15))
for i in range(k):
    pc_map = topkeigenvectors[:, i].reshape(28, 28)
    plt.subplot(5, 5, i + 1)
    plt.imshow(pc_map, cmap='gray')
    plt.axis('off')
    plt.title(f'PC {i + 1}')

plt.suptitle(f'PC Maps of top {k}')
plt.tight_layout()
plt.show()

"""##### Reconstruct the dataset using different dimensional representations. How do these look like? If you had to pick a dimension d that can be used for a downstream task where you need to classify the digits correctly, what would you pick and why?"""

k = 131
top_eigenvectors = sort_e_vectors[:, :k]
projected_data = X_centered @ top_eigenvectors
reconstructed_data = projected_data @ top_eigenvectors.T

reconstructed_data += mean_X
reconstructed_data = np.real(reconstructed_data)

print("Reconstructed dataset shape:", reconstructed_data.shape)
print(reconstructed_data[:5, :])

num = 10
plt.figure(figsize=(16, 16))
for i in range(num):
    image = reconstructed_data[i*100].reshape(28, 28)
    plt.subplot(2, 5, i + 1)
    plt.imshow(image, cmap='gray')
    plt.axis('off')
    plt.tight_layout()
    plt.title(f'{i + 1}')

"""### 2) You are given a data-set with 1000 data points each in R2 (cm dataset 2.csv)

##### Write a piece of code to implement the Llyod”s algorithm for the K-means problem with k = 2 . Try 5 different random initialization and plot the error function w.r.t iterations in each case. In each case, plot the clusters obtained in different colors.
"""

df = pd.read_csv('/content/cm_dataset_2.csv', names=['x1', 'x2'], header=None)
plt.plot(df['x1'], df['x2'], '+')
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

def llyods_algo(data, k=2, max_iters=100):
    centroids = data[np.random.choice(data.shape[0], k, replace=False)]
    err = []
    for i in range(max_iters):
        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
        cluster_assignments = np.argmin(distances, axis=1)
        error = np.sum([np.linalg.norm(data[j] - centroids[cluster_assignments[j]])**2 for j in range(data.shape[0])])
        err.append(error)

        new_centroids = np.array([data[cluster_assignments == j].mean(axis=0) for j in range(k)])

        if np.allclose(centroids, new_centroids):
            break

        centroids = new_centroids
    return centroids, cluster_assignments, err

F_values = []
for i in range(5):
  np.random.seed(i)
  data_points = df.values
  centroids, cluster_assignments, F_value = llyods_algo(data_points, k=2)
  F_values.append(F_value)

  # Grid points over dataset range
  x_min, x_max = data_points[:, 0].min() - 0.1, data_points[:, 0].max() + 0.1
  y_min, y_max = data_points[:, 1].min() - 0.1, data_points[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))

  grid_points = np.c_[xx.ravel(), yy.ravel()]
  distances = np.linalg.norm(grid_points[:, np.newaxis] - centroids, axis=2)
  grid_assignments = np.argmin(distances, axis=1)

  grid_assignments = grid_assignments.reshape(xx.shape)
  plt.figure(figsize=(10, 6))
  plt.contourf(xx, yy, grid_assignments, cmap='viridis', alpha=0.3)
  plt.scatter(data_points[:, 0], data_points[:, 1], c=cluster_assignments, cmap='viridis', edgecolor='k')
  plt.scatter(centroids[:, 0], centroids[:, 1], color='red', marker='X', s=100, label='Centroids')

  # Labeling
  plt.title(f'K-Means Clustering with Voronoi Regions: random initialization {i+1}')
  plt.xlabel('Feature1')
  plt.ylabel('Feature2')
  plt.legend()
  plt.show()


# Plotting the error
plt.figure(figsize=(10, 6))
for i, lst in enumerate(F_values):
    x_values = range(len(lst))
    plt.plot(x_values,lst, label=f'Initialization {i+1}')


plt.ylim(0, max(max(lst) for lst in F_values))
plt.xlabel('Iterations')
plt.ylabel('Error')
plt.title('Error vs Iterations')
plt.legend()
plt.show()

"""##### For each K = {2, 3, 4, 5}, Fix an arbitrary initialization and obtain cluster centers according to K-means algorithm using the fixed initialization. For each value of K, plot the Voronoi regions associated to each cluster center. (You can assume the minimum and maximum value in the data-set to be the range for each component of R2 )."""

def llyods_algo(data, k=2, max_iters=100):
    np.random.seed(5400)
    centroids = data[np.random.choice(data.shape[0], k, replace=False)]
    err = []
    for i in range(max_iters):
        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
        cluster_assignments = np.argmin(distances, axis=1)
        error = np.sum([np.linalg.norm(data[j] - centroids[cluster_assignments[j]])**2 for j in range(data.shape[0])])
        err.append(error)

        new_centroids = np.array([data[cluster_assignments == j].mean(axis=0) for j in range(k)])

        if np.allclose(centroids, new_centroids):  # comment these two lines, to be able to zoom out the graph
            break                                  #

        centroids = new_centroids
    return centroids, cluster_assignments, err


F_values = []
for i in range(2,6):
  data_points = df.values
  centroids, cluster_assignments, F_value = llyods_algo(data_points, k=i)
  F_values.append(F_value)
  # Generate grid points over the range of the data
  x_min, x_max = data_points[:, 0].min() - 0.1, data_points[:, 0].max() + 0.1
  y_min, y_max = data_points[:, 1].min() - 0.1, data_points[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))

  grid_points = np.c_[xx.ravel(), yy.ravel()]
  distances = np.linalg.norm(grid_points[:, np.newaxis] - centroids, axis=2)
  grid_assignments = np.argmin(distances, axis=1)
  grid_assignments = grid_assignments.reshape(xx.shape)

  # Voranoi
  plt.figure(figsize=(10, 6))
  plt.contourf(xx, yy, grid_assignments, cmap='viridis', alpha=0.3)
  plt.scatter(data_points[:, 0], data_points[:, 1], c=cluster_assignments, cmap='viridis', edgecolor='k')
  plt.scatter(centroids[:, 0], centroids[:, 1], color='red', marker='X', s=100, label='Centroids')

  # Labeling
  plt.title(f'K-Means Clustering with Voronoi Regions: k = {i}')
  plt.xlabel('Feature1')
  plt.ylabel('Feature2')
  plt.legend()
  plt.show()


# Plotting the error
plt.figure(figsize=(10, 6))
for i, lst in enumerate(F_values):
    x_values = range(len(lst))
    plt.plot(x_values,lst, label=f'K = {i+2}')
plt.ylim(0, max(max(lst) for lst in F_values))
plt.xlabel('Iterations')
plt.ylabel('Error')
plt.title('Error vs Iterations')
plt.legend()
plt.show()

"""##### Is the Llyod’s algorithm a good way to cluster this dataset? If yes, justify your answer. If not, give your thoughts on what other procedure would you recommendto cluster this dataset?"""