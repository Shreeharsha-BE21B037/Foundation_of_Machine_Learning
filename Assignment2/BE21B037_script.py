# -*- coding: utf-8 -*-
"""FML_assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s2RZ1h6zE86i2OkhI90rBcPBBlIbzLgs
"""

# Import libraries
import pandas as pd
import numpy as np
import os
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

# Available datasets
df = pd.read_csv("/content/emails.csv") # File Path
df1, df2 = train_test_split(df, test_size=0.3, stratify=df['spam'], random_state=42)
print(len(df1))
df

df1.groupby('spam').describe()

df2.groupby('spam').describe()

# Training Data
x_train = df1['text']
y_train = df1['spam']

# Binary Count Matrix
cv = CountVectorizer()
count_matrix = cv.fit_transform(x_train.values)
encoded_df = pd.DataFrame(count_matrix.toarray(), columns=cv.get_feature_names_out()) # Count matrix stored as DataFrame

# Create Laplacian smoothing datapoints
Laplacian_spam = pd.DataFrame([[1] * encoded_df.shape[1]], columns=encoded_df.columns)
Laplacian_ham = pd.DataFrame([[1] * encoded_df.shape[1]], columns=encoded_df.columns)
y_train =  pd.concat([y_train, pd.DataFrame([[0], [1]], columns=[0])], ignore_index=True)

# Concatnate Smoothing datapoints and training data
encoded_df = pd.concat([encoded_df, Laplacian_spam], ignore_index=True)
encoded_df = pd.concat([encoded_df, Laplacian_ham], ignore_index=True)

# Split Training data based on spam or ham
encoded_df['Binary'] = y_train
spam_df = encoded_df[encoded_df['Binary'] == 1]
ham_df = encoded_df[encoded_df['Binary'] == 0]

# Spam and Ham Inputs
spam_x = spam_df.iloc[:,:-1]
ham_x = ham_df.iloc[:,:-1]

# Maximum likelihood estimation calculation for each feature
spam_sum = pd.DataFrame(spam_x.sum()).T
ham_sum = pd.DataFrame(ham_x.sum()).T
spam_prob = spam_sum.div(len(spam_x),axis=1)
ham_prob = ham_sum.div(len(ham_x),axis = 1)
p_spam = len(spam_x)/(len(spam_x)+len(ham_x))
p_ham = len(ham_x)/(len(spam_x)+len(ham_x))

# Prediction (Given text is spam or ham)
def test_email(text, cv, spam_prob, ham_prob, p_spam, p_ham):
    l = spam_prob.shape[1]
    test_wordcount = cv.transform([text]) # Any new word that is not present in the training dictionary is ignored.
    test_df = pd.DataFrame(test_wordcount.toarray(), columns=cv.get_feature_names_out())

    new_df = test_df * np.log((spam_prob * (1 - ham_prob))/(ham_prob * (1 - spam_prob))) + np.log((1-spam_prob)/(1-ham_prob))
    sum = new_df.sum().sum()
    sum = sum + (np.log(p_spam/p_ham))

    if sum >= 0:
        return 1 # Spam
    else :
        return 0 # Ham

# Testing
[TP,TN,FN,FP] = [0,0,0,0]

for index,row in df2.iterrows():
    text = row['text']
    actual = row['spam']
    predicted = test_email(text, cv, spam_prob, ham_prob, p_spam, p_ham)

    if actual == predicted == 1:
        TP +=1
    elif actual == predicted == 0:
        TN += 1
    elif actual == 1 and predicted == 0:
        FN += 1
    elif actual == 0 and predicted == 1:
        FP += 1

Accuracy = (TP + TN)/(TP + TN + FP + FN)
Recall = TP/ (TP + FN)
Precision = TP/ (TP + FP)
F1_score = TP / (TP + 0.5*(FP + FN))
F1 = 2*(Precision*Recall)/(Recall+Precision)

print(f'Accuracy = {Accuracy}')
print(f'Recall = {Recall}')
print(f'Precision = {Precision}')
print(f'F1_score = {F1_score}')

# Reading the test data
folder_path = "/content/test"  # input path of Test Folder
check_df = pd.DataFrame(columns=["email", "spam/ham"])

for filename in os.listdir(folder_path):
    if filename.endswith(".txt"):
        file_path = os.path.join(folder_path, filename)

        with open(file_path, 'r') as file:
            file_content = file.read()

        x = test_email(file_content, cv, spam_prob, ham_prob, p_spam, p_ham)
        check_df.loc[len(check_df)] = [filename, x]

        if x == 1:
            print(f"{filename} is a spam email")
        else:
            print(f"{filename} is not a spam email")
        print("\n" + "-"*40 + "\n")

# print(check_df)
check_df.to_csv('SpamHam.csv', index=False)