# -*- coding: utf-8 -*-
"""Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G3cCDUlvLwNHyT8ZOHO42ppcFd0cPLBF
"""

import pandas as pd               # Data Handling
import numpy as np                # Data manipulation
from numpy.linalg import inv      # Calculate Matrix Inverse
import matplotlib.pyplot as plt   # Plot Graphs
import plotly.express as px       # Plot 3D figures to visualize data
import random                     # To randomize
import statistics                 # To calculate central tendencies
import warnings                   # To ignore warnings
warnings.filterwarnings('ignore')


train = pd.read_csv("/content/FMLA1Q1Data_train.csv",header = None)
test = pd.read_csv("/content/FMLA1Q1Data_test.csv",header = None)
train.columns = ["x1","x2","y"]
test.columns = ["x1","x2","y"]


XT = np.matrix(train[["x1","x2"]])
Y = np.matrix(train[["y"]])
X = XT.transpose()
XXT = np.dot(X,XT)
XXT_inv = inv(XXT)
XXT_invX = np.dot(XXT_inv,X)
Wml = np.dot(XXT_invX,Y)
print(f'The analytical solution, gives the ideal weights as \n w1 = {float(Wml[0])} and w2 = {float(Wml[1])}' )


# Gradient descent Algorithm
random.seed(5400)
Wt = np.matrix([[random.randint(1, 10)],[random.randint(1, 10)]])
XXT = np.dot(X,XT)
XY = np.dot(X,Y)
err_func = []

for t in range(1,100):
    n =  1/(10000)
    gradf = np.dot(XXT,Wt)-XY             # gradient of least square
    err = float(np.linalg.norm(Wt - Wml)) # Error measurement
    err_func.append(err)
    Wt1 = Wt - (n*2*gradf)
    Wt = Wt1

y = [i for i in range(1,100)]

plt.plot(y,err_func)
plt.xlabel("Time steps")
plt.ylabel('$||W^t - W_{ML}||_2$')
plt.xlim(0,100)
plt.title("$||W^t - W_{ML}||_2$ vs time")
plt.savefig("GD.png")
plt.show()

print(f'The weights after 100 steps of gradient descent algorithm, the weights are w1 = {float(Wt[0])} and w2 = {float(Wt[1])}, with an error of {np.linalg.norm(Wt-Wml)}')